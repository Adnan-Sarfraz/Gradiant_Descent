# -*- coding: utf-8 -*-
"""Gradiant-descent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B-TEt_Pxd3XMu_uk1TjFQ-WUmUpMaMRA
"""

import numpy as np
  import matplotlib.pyplot as plt
  X = np.array([[1, 5, 2],
                [1, 7, 3],
                [1, 3, 1],
                [1, 10, 3],
                [1, 6, 2]])
  Y = np.array([60, 75, 50, 90, 65])
  m = len(Y)
  alpha = 0.01
  iterations = 50
  theta = np.zeros(3)
  def gradient_descent(X, Y, theta, alpha, iterations):
      cost_history = []
      for _ in range(iterations):
          predictions = np.dot(X, theta)
          errors = predictions - Y
          gradient = (1/m) * np.dot(X.T, errors)
          theta -= alpha * gradient
          cost = (1/(2*m)) * np.sum(errors**2)
          cost_history.append(cost)
      return theta, cost_history
  theta, cost_history = gradient_descent(X, Y, theta, alpha, iterations)
  plt.plot(range(iterations), cost_history, marker='o')
  plt.title("Cost Function over Iterations")
  plt.xlabel("Iterations")
  plt.ylabel("Cost")
  plt.grid()
  plt.show()
  print("Optimized Parameters (Theta):", theta)

import numpy as np
import matplotlib.pyplot as plt
m = len(Y)
alpha = 0.01
iterations = 50
X = np.array([[1, 5, 2],
              [1, 7, 3],
              [1, 3, 1],
              [1, 10, 3],
              [1, 6, 2]])
Y = np.array([60, 75, 50, 90, 65])


# Initialize theta
theta = np.zeros(3)
# Gradient Descent Function
def gradient_descent(X, Y, theta, alpha, iterations):
    cost_history = []
    for _ in range(iterations):
        predictions = np.dot(X, theta)
        errors = predictions - Y
        gradient = (1/m) * np.dot(X.T, errors)
        theta -= alpha * gradient
        cost = (1/(2*m)) * np.sum(errors**2)
        cost_history.append(cost)
    return theta, cost_history
theta, cost_history = gradient_descent(X, Y, theta, alpha, iterations)
# Plot the cost function
plt.plot(range(iterations), cost_history, marker='o')
plt.title("Cost Function Over Iterations")
plt.xlabel("Iterations")
plt.ylabel("Cost")
plt.grid()
plt.show()
# Final Output
print("Final Optimized Parameters (Theta):", theta)
print("Final Cost:", cost_history[-1])